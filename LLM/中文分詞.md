from transformers import BertTokenizer, BertModel

# 加載BERT中文分詞器和模型
model_name = 'bert-base-chinese'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

#3. 數據準備 準備你的中文文本數據，并進行分詞處理。可以使用分詞器將文本轉換爲模型可以理解的輸入格式。
示例：文本編碼
sentences = ["白日依山盡", "黃河入海流"] # 準備中文句子

inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt') # 使用分詞器進行編碼

#4. 模型推理 將編碼後的輸入傳遞給模型，獲取輸出。 示例：獲取模型輸出
with torch.no_grad():  # 不計算梯度
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state # 獲取最後一層隱藏狀態
print(last_hidden_states)

#5. 微調模型（可選） 如果你有特定的任務（如文本分類、命名實體識別等），可以在此基礎上進行微調。你需要準備好標注數據，并使用適當的損失函數和優化器進行訓練。
from transformers import Trainer, TrainingArguments

# 定義訓練參數
training_args = TrainingArguments( output_dir='./results', num_train_epochs=3, per_device_train_batch_size=8, save_steps=10_000, save_total_limit=2)

# 創建Trainer實例并開始訓練
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset,  # 你的訓練數據集)

trainer.train()

#6. 保存和加載模型 訓練完成後，可以保存微調後的模型，以便後續使用。 示例：保存和加載模型
model.save_pretrained('./my_model') # 保存模型
tokenizer.save_pretrained('./my_model')

model = BertModel.from_pretrained('./my_model') # 加載模型
tokenizer = BertTokenizer.from_pretrained('./my_model')

#通過以上步驟，你可以在Hugging Face上建立并使用中文語言模型。這些步驟涵蓋了從環境設置到模型推理及微調的全過程。根據具體需求，你可以調整數據集和訓練參數
