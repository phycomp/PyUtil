== Pytorch å¦‚ä½•é¸æ“‡ optim ==
åœ¨ PyTorch ä¸­ï¼Œé¸æ“‡åˆé©çš„å„ªåŒ–å™¨ï¼ˆoptimï¼‰å–æ±ºäºä½ çš„æ¨¡å‹é¡å‹ã€è¨“ç·´ç›®æ¨™ä»¥åŠæ•¸æ“šçš„æ€§è³ªã€‚å¸¸è¦‹çš„å„ªåŒ–å™¨åŒ…æ‹¬ SGDã€Adamã€RMSprop ç­‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è¦‹å„ªåŒ–å™¨çš„é¸æ“‡æ¨™æº–ï¼š

1. SGD (Stochastic Gradient Descent)
é©ç”¨å ´æ™¯ï¼šç¶“å…¸çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ã€‚ç•¶ä½ éœ€è¦æ›´é«˜çš„è¨“ç·´ç©©å®šæ€§ä¸”æ„¿æ„æ‰‹å‹•èª¿æ•´å­¸ç¿’ç‡æ™‚ï¼Œé©ç”¨å°å‹å’Œç°¡å–®çš„æ¨¡å‹ã€‚
ä½¿ç”¨æŠ€å·§ï¼šå¯ä»¥æ­é… momentum åƒæ•¸åŠ é€Ÿæ”¶æ–‚å¹¶é¿å…å±€éƒ¨æœ€å„ªè§£ã€‚é©ç”¨äºå¤§å¤šæ•¸ç¶“å…¸çš„è¨ˆç®—ä»»å‹™ï¼Œä½†å­¸ç¿’ç‡çš„é¸æ“‡éœ€è¦è¬¹æ…èª¿æ•´ã€‚
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
2. Adam (Adaptive Moment Estimation)
é©ç”¨å ´æ™¯ï¼šå°å­¸ç¿’ç‡æ•æ„Ÿåº¦è¼ƒä½ï¼Œé©åˆå¤§å¤šæ•¸æ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚é€šå¸¸è¡¨ç¾å‡ºè¼ƒå¿«çš„æ”¶æ–‚é€Ÿåº¦ã€‚é©ç”¨äºè¼ƒå¤§ä¸”è¤‡é›œçš„ç¥ç¶“ç¶²çµ¡ï¼Œå¦‚å·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆCNNsï¼‰æˆ–å¾ªç’°ç¥ç¶“ç¶²çµ¡ï¼ˆRNNsï¼‰ã€‚
ä½¿ç”¨æŠ€å·§ï¼šé»˜èªåƒæ•¸ lr=0.001 é€šå¸¸èƒ½ç²å¾—ä¸éŒ¯çš„æ•ˆæœï¼Œç›¡ç®¡æœ‰æ™‚éœ€è¦é€²ä¸€æ­¥èª¿æ•´ã€‚Adam æ˜¯å¤§éƒ¨åˆ†æ·±åº¦å­¸ç¿’ä»»å‹™çš„é¦–é¸å„ªåŒ–å™¨ã€‚
ç¼ºé»ï¼šæœ‰æ™‚å¯èƒ½æœƒéè—•åˆæˆ–ä¸æ˜“æ”¶æ–‚åˆ°å…¨å±€æœ€å„ªè§£ã€‚
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
3. RMSprop
é©ç”¨å ´æ™¯ï¼šé¡ä¼¼äº Adamï¼Œé©ç”¨äºè™•ç†éå¹³ç©©ç›®æ¨™ã€RNN ç­‰åºåˆ—æ•¸æ“šã€‚å°ç¨€ç–æ¢¯åº¦æœ‰æ›´å¥½çš„è¡¨ç¾ã€‚
ä½¿ç”¨æŠ€å·§ï¼šé€šå¸¸ç”¨äºè™•ç†æ™‚é–“åºåˆ—æ•¸æ“šæˆ–è¨“ç·´å¾ªç’°ç¥ç¶“ç¶²çµ¡ã€‚
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)
4. AdamW
é©ç”¨å ´æ™¯ï¼šæ˜¯ Adam çš„æ”¹é€²ç‰ˆï¼Œæ‡‰ç”¨æ¬Šé‡è¡°æ¸›ä¾†æ­£å‰‡åŒ–æ¨¡å‹ã€‚é©ç”¨äºéœ€è¦è¼ƒå¼·æ­£å‰‡åŒ–çš„å ´æ™¯ï¼Œå°¤å…¶æ˜¯ Transformer é¡æ¨¡å‹ã€‚
ä½¿ç”¨æŠ€å·§ï¼šæ¬Šé‡è¡°æ¸›å¯ä»¥é˜²æ­¢æ¨¡å‹éè—•åˆã€‚
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
5. å…¶ä»–é¸æ“‡
- Adagradï¼šé©åˆç¨€ç–æ•¸æ“šçš„è™•ç†ï¼Œä½†åœ¨è™•ç†æŸäº›æ·±åº¦å­¸ç¿’æ¨¡å‹æ™‚è¡¨ç¾ä¸å¦‚ Adamã€‚
- Adadeltaï¼šå°å­¸ç¿’ç‡æ›´ä¸æ•æ„Ÿï¼Œé©ç”¨äºæ›´è¤‡é›œçš„æ¨¡å‹ã€‚
- LBFGSï¼šé©ç”¨äºå°æ‰¹é‡è¨“ç·´çš„æ•¸æ“šé›†ï¼Œä½†ç”±äºè¨ˆç®—æˆæœ¬é«˜ï¼Œé€šå¸¸ç”¨äºå°è¦æ¨¡å•é¡Œã€‚
- é¸æ“‡å„ªåŒ–å™¨çš„å»ºè­°ï¼š
- ç°¡å–®æ¨¡å‹ï¼šå¦‚æœæ¨¡å‹è¼ƒç°¡å–®æˆ–æ•¸æ“šé›†è¼ƒå°ï¼ŒSGD æˆ– Adam éƒ½æ˜¯ä¸éŒ¯çš„é¸æ“‡ã€‚
- è¤‡é›œæ¨¡å‹ï¼šå°äºå¤§è¦æ¨¡è¤‡é›œçš„æ¨¡å‹æˆ–ç¥ç¶“ç¶²çµ¡ï¼ŒAdam é€šå¸¸æ˜¯é¦–é¸ã€‚
- RNN æˆ–åºåˆ—æ¨¡å‹ï¼šRMSprop æˆ– Adam é€šå¸¸è¡¨ç¾è¼ƒå¥½ã€‚
- æ­£å‰‡åŒ–éœ€æ±‚é«˜ï¼šä½¿ç”¨ AdamWã€‚
å¯ä»¥æ ¹æ“šæ¨¡å‹çš„è¨“ç·´è¡¨ç¾å’Œæ”¶æ–‚æƒ…æ³ä¾†èª¿æ•´å„ªåŒ–å™¨å’Œç›¸é—œåƒæ•¸ï¼Œä»¥é”åˆ°æœ€ä½³æ•ˆæœã€‚

== Pytorch æ¢¯åº¦æ–¹å‘ gradient ==
åœ¨ PyTorch ä¸­ï¼Œæ¢¯åº¦ä»£è¡¨æå¤±å‡½æ•¸ç›¸å°äºæ¨¡å‹åƒæ•¸çš„åå°æ•¸ï¼Œå³æå¤±å‡½æ•¸å°æ¯å€‹åƒæ•¸è®ŠåŒ–çš„æ•æ„Ÿåº¦ã€‚æ¢¯åº¦çš„æ–¹å‘æŒ‡å‘æå¤±å‡½æ•¸ä¸Šå‡æœ€å¿«çš„æ–¹å‘ï¼Œè€Œå„ªåŒ–å™¨å‰‡æ ¹æ“šæ¢¯åº¦çš„è² æ–¹å‘ï¼ˆå³æå¤±å‡½æ•¸æ¸›å°æœ€å¿«çš„æ–¹å‘ï¼‰æ›´æ–°æ¨¡å‹çš„åƒæ•¸ã€‚

PyTorch ä¸­æ¢¯åº¦çš„è¨ˆç®— PyTorch çš„è‡ªå‹•å¾®åˆ†å¼•æ“ï¼ˆautogradï¼‰èƒ½å¤ é€šéåå‘å‚³æ’­è¨ˆç®—å‡ºæ¢¯åº¦ã€‚é—œéµçš„æ­¥é©ŸåŒ…æ‹¬ï¼š

å‰å‘å‚³æ’­ï¼šé€šéæ¨¡å‹è¨ˆç®—å‡ºæå¤±å‡½æ•¸å€¼ã€‚
åå‘å‚³æ’­ï¼šé€šé loss.backward() è¨ˆç®—æå¤±ç›¸å°äºæ¯å€‹åƒæ•¸çš„æ¢¯åº¦ã€‚æ¯å€‹åƒæ•¸çš„æ¢¯åº¦æœƒè¢«å­˜å„²åœ¨ parameter.grad ä¸­ã€‚
æ›´æ–°æ¬Šé‡ï¼šå„ªåŒ–å™¨æ ¹æ“šé€™äº›æ¢¯åº¦ä¾†æ›´æ–°åƒæ•¸ï¼Œå…¸å‹çš„æ›´æ–°è¦å‰‡çˆ²ï¼š
from torch.nn import Linear, MSELoss
from torch.optim import SGD

# å®šç¾©ç°¡å–®çš„æ¨¡å‹
model = Linear(2, 1)  # ç·šæ€§å±¤è¼¸å…¥2ç¶­ï¼Œè¼¸å‡º1ç¶­
optimizer = SGD(model.parameters(), lr=0.01)

from torch import tensor as trchTnsr
è¼¸å…¥ = trchTnsr([[1.0, 2.0]], requires_grad=True)  # è¼¸å…¥ # å‡è¨­æœ‰ä¸€å€‹ç°¡å–®çš„è¼¸å…¥å’Œæ¨™ç°½
ç›®æ¨™ = trchTnsr([[1.0]])  # ç›®æ¨™è¼¸å‡º

# å‰å‘å‚³æ’­ï¼šè¨ˆç®—è¼¸å‡ºå’Œæå¤±
è¼¸å‡º = model(**è¼¸å…¥)
criterion = MSELoss()  # ä½¿ç”¨å‡æ–¹èª¤å·®æå¤±å‡½æ•¸
loss = criterion(è¼¸å‡º, ç›®æ¨™)

loss.backward() # åå‘å‚³æ’­ï¼šè¨ˆç®—æ¢¯åº¦

for param in model.parameters(): # æŸ¥çœ‹æ¢¯åº¦æ–¹å‘
    print(param.grad)  # é€™æ˜¯æ¢¯åº¦å€¼

optimizer.step() # ä½¿ç”¨å„ªåŒ–å™¨æ›´æ–°æ¬Šé‡

optimizer.zero_grad() # æ¸…é™¤æ¢¯åº¦ç·©å­˜ï¼ˆå› çˆ² PyTorch æœƒç´¯ç©æ¢¯åº¦ï¼‰
requires_grad=Trueï¼šåœ¨å®šç¾©è¼¸å…¥æˆ–æ¨¡å‹åƒæ•¸æ™‚ï¼Œå¦‚æœæŸå€‹å¼µé‡éœ€è¦è¨ˆç®—æ¢¯åº¦ï¼Œå¿…é ˆå°‡ requires_grad=Trueã€‚
loss.backward()ï¼šåå‘å‚³æ’­éç¨‹ä¸­ï¼ŒPyTorch æœƒè‡ªå‹•è¨ˆç®—æå¤±ç›¸å°äºæ¯å€‹åƒæ•¸çš„æ¢¯åº¦ï¼Œå¹¶å­˜å„²åœ¨ parameter.grad ä¸­ã€‚
optimizer.step()ï¼šé€šéè¨ˆç®—å‡ºçš„æ¢¯åº¦ï¼Œå„ªåŒ–å™¨æœƒæ›´æ–°æ¨¡å‹åƒæ•¸ã€‚
optimizer.zero_grad()ï¼šçˆ²äº†é˜²æ­¢æ¢¯åº¦ç´¯ç©ï¼Œæ¯æ¬¡åå‘å‚³æ’­å¾Œéœ€è¦æ‰‹å‹•æ¸…é›¶æ¢¯åº¦ã€‚
æ¢¯åº¦æ–¹å‘çš„æ„ç¾© æ­£æ¢¯åº¦ï¼šæ„å‘³è‘—å¦‚æœåƒæ•¸åœ¨é€™å€‹æ–¹å‘ä¸Šå¢åŠ ï¼Œæå¤±ä¹Ÿæœƒå¢åŠ ã€‚å› æ­¤ï¼Œæ›´æ–°æ™‚æ‡‰æ²¿è‘—æ¢¯åº¦çš„åæ–¹å‘ç§»å‹•ï¼Œä»¥æ¸›å°æå¤±ã€‚
è² æ¢¯åº¦ï¼šè¡¨ç¤ºåœ¨é€™å€‹æ–¹å‘ä¸Šæ¸›å°‘åƒæ•¸çš„å€¼æœƒå¢å¤§æå¤±ï¼Œå› æ­¤éœ€è¦å¢åŠ åƒæ•¸çš„å€¼ã€‚
é€šå¸¸ï¼Œæ¢¯åº¦çš„æ–¹å‘æŒ‡å‘æå¤±å‡½æ•¸å¢é•·æœ€å¿«çš„æ–¹å‘ï¼Œè€Œå„ªåŒ–çš„ç›®çš„æ˜¯æ²¿è‘—è² æ¢¯åº¦æ–¹å‘ï¼Œé€æ­¥æ¸›å°æå¤±å‡½æ•¸çš„å€¼ã€‚

== Pytorch å¦‚ä½•é¸æ“‡ æå¤±å‡½æ•¸ ==
åœ¨ PyTorch ä¸­ï¼Œé¸æ“‡åˆé©çš„æå¤±å‡½æ•¸ï¼ˆLoss Functionï¼‰å°æ¨¡å‹çš„æ€§èƒ½å’Œè¨“ç·´éç¨‹è‡³é—œé‡è¦ã€‚æå¤±å‡½æ•¸ç”¨äºè¡¡é‡æ¨¡å‹é æ¸¬çµæœèˆ‡çœŸå¯¦æ¨™ç°½ä¹‹é–“çš„å·®ç•°ï¼Œå®ƒæœƒå½±éŸ¿æ¢¯åº¦çš„è¨ˆç®—ï¼Œé€²è€Œå½±éŸ¿æ¨¡å‹çš„åƒæ•¸æ›´æ–°ã€‚é¸æ“‡æå¤±å‡½æ•¸æ™‚éœ€è¦è€ƒæ…®ä»»å‹™çš„é¡å‹ï¼Œå¦‚åˆ†é¡ã€å›æ­¸ã€æˆ–ç”Ÿæˆä»»å‹™ã€‚

1. åˆ†é¡å•é¡Œ
äº¤å‰ç†µæå¤±ï¼ˆCrossEntropyLossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºå¤šåˆ†é¡å•é¡Œï¼Œé€šå¸¸æ­é… Softmax è¼¸å‡ºå±¤ä½¿ç”¨ã€‚
å…¬å¼ï¼š
  æ˜¯çœŸå¯¦æ¨™ç°½ï¼Œ
  æ˜¯æ¨¡å‹é æ¸¬çš„æ¦‚ç‡ã€‚
ç‰¹é»ï¼š
äº¤å‰ç†µæå¤±æœƒè‡ªå‹•è¨ˆç®— Log Softmaxï¼Œå› æ­¤ä¸éœ€è¦é¡¯å¼åœ°åœ¨ç¶²çµ¡ä¸­åŠ å…¥ Softmax å±¤ã€‚
é©ç”¨äºç¨å é¡åˆ¥çš„å¤šåˆ†é¡ä»»å‹™ï¼Œä¾‹å¦‚åœ–åƒåˆ†é¡ã€‚
import torch.nn as nn

loss_fn = nn.CrossEntropyLoss()
äºŒå…ƒäº¤å‰ç†µæå¤±ï¼ˆBCELoss / BCEWithLogitsLossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºäºŒåˆ†é¡å•é¡Œã€‚
å…¬å¼ï¼š
L=âˆ’[ylog(p)+(1âˆ’y)log(1âˆ’p)]
å…¶ä¸­
ğ‘¦
y æ˜¯çœŸå¯¦æ¨™ç°½ï¼Œ
ğ‘
p æ˜¯æ¨¡å‹é æ¸¬çš„æ¦‚ç‡ã€‚
BCEWithLogitsLossï¼šé©ç”¨äºäºŒåˆ†é¡å•é¡Œï¼Œå®ƒé›†æˆäº† Sigmoid å‡½æ•¸å’ŒäºŒå…ƒäº¤å‰ç†µæå¤±ï¼Œè¨ˆç®—æ›´çˆ²ç©©å®šã€‚
loss_fn = nn.BCEWithLogitsLoss()  # æ›´æ¨è–¦çš„ç‰ˆæœ¬
2. å›æ­¸å•é¡Œ
å‡æ–¹èª¤å·®æå¤±ï¼ˆMSELossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºå›æ­¸ä»»å‹™ï¼Œé æ¸¬çš„æ˜¯é€£çºŒæ•¸å€¼ã€‚
å…¬å¼ï¼š
  æ˜¯çœŸå¯¦å€¼ï¼Œ
  æ˜¯æ¨¡å‹çš„é æ¸¬å€¼ã€‚
ç‰¹é»ï¼š
å‡æ–¹èª¤å·®å°ç•°å¸¸å€¼ï¼ˆoutliersï¼‰è¼ƒæ•æ„Ÿã€‚
loss_fn = nn.MSELoss()
å¹³å‡çµ•å°èª¤å·®æå¤±ï¼ˆL1Lossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºå›æ­¸ä»»å‹™ï¼Œè¨ˆç®—é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“çš„çµ•å°å·®ã€‚
å…¬å¼ï¼š
ç‰¹é»ï¼š å°ç•°å¸¸å€¼ä¸å¤ªæ•æ„Ÿï¼Œç›¸è¼ƒäº MSE æå¤±ï¼ŒL1 æ›´é­¯æ£’ã€‚
loss_fn = nn.L1Loss()
3. ç”Ÿæˆä»»å‹™ï¼ˆå¦‚åœ–åƒç”Ÿæˆã€GANsï¼‰
å°æŠ—æå¤±ï¼ˆAdversarial Lossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºç”Ÿæˆå°æŠ—ç¶²çµ¡ï¼ˆGANsï¼‰ä¸­ï¼Œè¨“ç·´ç”Ÿæˆå™¨å’Œåˆ¤åˆ¥å™¨ã€‚
ç‰¹é»ï¼š åˆ¤åˆ¥å™¨ä½¿ç”¨äº¤å‰ç†µæå¤±æˆ– BCELossï¼Œç”Ÿæˆå™¨å‰‡ä½¿ç”¨èˆ‡å…¶å°æŠ—çš„æå¤±ã€‚
4. åºåˆ—é æ¸¬å•é¡Œï¼ˆå¦‚èªè¨€æ¨¡å‹ã€ç¿»è­¯ç­‰ï¼‰
CTC æå¤±ï¼ˆConnectionist Temporal Classification, CTC Lossï¼‰
é©ç”¨å ´æ™¯ï¼šç”¨äºç„¡å°é½Šæ¨™æ³¨çš„åºåˆ—ä»»å‹™ï¼Œå¦‚èªéŸ³è­˜åˆ¥ã€æ‰‹å¯«è­˜åˆ¥ç­‰ã€‚
ç‰¹é»ï¼š è§£æ±ºè¼¸å…¥åºåˆ—èˆ‡è¼¸å‡ºæ¨™ç°½ä¸ç­‰é•·ä¸”æœªå°é½Šçš„å•é¡Œã€‚
loss_fn = nn.CTCLoss()
5. è‡ªå®šç¾©æå¤±å‡½æ•¸
å¦‚æœ PyTorch å…§ç½®çš„æå¤±å‡½æ•¸ä¸æ»¿è¶³éœ€æ±‚ï¼Œå¯ä»¥è‡ªå®šç¾©æå¤±å‡½æ•¸ã€‚

import torch
import torch.nn as nn

class CustomLoss(nn.Module):
    def __init__(self):
        super(CustomLoss, self).__init__()

    def forward(self, predictions, targets):
        loss = torch.mean((predictions - targets)**2)  # ç°¡å–®çš„ MSE ç¤ºä¾‹
        return loss

loss_fn = CustomLoss()
å¦‚ä½•é¸æ“‡æå¤±å‡½æ•¸ï¼Ÿ
åˆ†é¡å•é¡Œï¼š

äºŒåˆ†é¡ï¼šBCELoss æˆ– BCEWithLogitsLossã€‚
å¤šåˆ†é¡ï¼šCrossEntropyLossã€‚
å›æ­¸å•é¡Œï¼š

å¦‚æœé æ¸¬çš„æ˜¯é€£çºŒæ•¸å€¼ï¼šMSELoss æˆ– L1Lossã€‚
ç‰¹æ®Šçµæ§‹ä»»å‹™ï¼š

åºåˆ—é æ¸¬ï¼šCTCLossã€‚
ç”Ÿæˆä»»å‹™ï¼šç”Ÿæˆå°æŠ—ç¶²çµ¡ï¼ˆGANsï¼‰ä½¿ç”¨å°æŠ—æå¤±ã€‚
æå¤±å‡½æ•¸çš„èª¿è©¦èˆ‡æ¬Šè¡¡ï¼š

å°ç•°å¸¸å€¼æ•æ„Ÿï¼šMSELoss é©åˆæ¨¡å‹é æ¸¬å€¼è®ŠåŒ–ç¯„åœè¼ƒå°çš„æƒ…æ³ã€‚
å°ç•°å¸¸å€¼ä¸æ•æ„Ÿï¼šL1Loss æ›´é©åˆç©©å¥çš„å›æ­¸ä»»å‹™ã€‚
æå¤±å‡½æ•¸çš„é¸æ“‡èˆ‡æ¨¡å‹è¨“ç·´ç›®æ¨™å¯†åˆ‡ç›¸é—œï¼Œæ‡‰æ ¹æ“šå…·é«”ä»»å‹™çš„æ€§è³ªé€²è¡Œèª¿æ•´å’Œè©¦é©—ã€‚



************************************************
Pytorch ä¸­æ–‡æ–·è© é¸æ“‡æå¤±å‡½æ•¸ åœ¨ PyTorch ä¸­ï¼Œè™•ç†ä¸­æ–‡æ–‡æœ¬çš„åˆ†è©ä»»å‹™æ™‚ï¼Œæå¤±å‡½æ•¸çš„é¸æ“‡ä¸»è¦å–æ±ºäºæ¨¡å‹çš„ç›®æ¨™å’Œä»»å‹™é¡å‹ã€‚å¦‚æœä½ çš„ä»»å‹™æ˜¯ä¸­æ–‡æ–‡æœ¬çš„åˆ†è©å•é¡Œï¼Œå¯ä»¥å°‡å…¶è¦–çˆ²ä¸€ç¨®åºåˆ—æ¨™æ³¨å•é¡Œï¼ˆsequence labelingï¼‰ï¼Œå…¶ä¸­æ¯å€‹å­—ç¬¦å°æ‡‰ä¸€å€‹æ¨™ç°½ï¼ˆä¾‹å¦‚æ˜¯å¦æ˜¯è©çš„é–‹é ­ã€ä¸­é–“æˆ–çµå°¾ï¼‰ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•é¸æ“‡é©åˆä¸­æ–‡åˆ†è©ä»»å‹™çš„æå¤±å‡½æ•¸ã€‚

1. ä»»å‹™é¡å‹
ä¸­æ–‡åˆ†è©å¯ä»¥ç†è§£çˆ²å°æ¯å€‹å­—ç¬¦é€²è¡Œåˆ†é¡ï¼Œå…¸å‹çš„æ¨™ç°½é›†å¯èƒ½åŒ…æ‹¬ï¼š

Bï¼ˆBeginningï¼‰ï¼šè¡¨ç¤ºè©çš„é–‹é ­
Iï¼ˆInsideï¼‰ï¼šè¡¨ç¤ºè©çš„ä¸­é–“éƒ¨åˆ†
Eï¼ˆEndï¼‰ï¼šè¡¨ç¤ºè©çš„çµå°¾
Sï¼ˆSingleï¼‰ï¼šè¡¨ç¤ºå–®å­—è©
é€™ç¨®æƒ…æ³å¯ä»¥å°‡åˆ†è©ä»»å‹™è¦–çˆ²å¤šåˆ†é¡å•é¡Œï¼Œå³æ¯å€‹å­—ç¬¦å°æ‡‰ä¸€å€‹æ¨™ç°½ã€‚

2. æå¤±å‡½æ•¸é¸æ“‡
ç”±äºä¸­æ–‡åˆ†è©æ˜¯ä¸€å€‹å…¸å‹çš„åºåˆ—æ¨™æ³¨ä»»å‹™ï¼Œæ¯å€‹å­—ç¬¦å¯ä»¥æœ‰å¤šå€‹é¡åˆ¥ï¼Œå› æ­¤é©åˆçš„æå¤±å‡½æ•¸é€šå¸¸æ˜¯å¤šåˆ†é¡ä»»å‹™å¸¸ç”¨çš„ CrossEntropyLoss æˆ–è€…åœ¨æŸäº›æƒ…æ³ä¸‹ä½¿ç”¨ CTCLossã€‚ä»¥ä¸‹æ˜¯å¹¾ç¨®å¯èƒ½çš„é¸æ“‡ï¼š

1. CrossEntropyLoss
é©ç”¨å ´æ™¯ï¼šé©åˆæ¯å€‹å­—ç¬¦çš„ç¨ç«‹å¤šåˆ†é¡å ´æ™¯ï¼Œé€šå¸¸ç”¨äº BERTã€LSTMã€CRF ç­‰æ¨¡å‹çš„è¼¸å‡ºã€‚
ç‰¹é»ï¼š
å°‡æ¯å€‹å­—ç¬¦çš„åˆ†è©æ¨™ç°½ä½œçˆ²åˆ†é¡å•é¡Œï¼Œæå¤±å‡½æ•¸è¨ˆç®—æ¯å€‹å­—ç¬¦åˆ†é¡çš„äº¤å‰ç†µæå¤±ã€‚
å¦‚æœæ¨¡å‹ç›´æ¥è¼¸å‡ºæ¯å€‹å­—ç¬¦çš„æ¨™ç°½é¡åˆ¥ï¼ˆå¦‚ Bã€Iã€Eã€Sï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ CrossEntropyLossã€‚
import torch.nn as nn

# å®šç¾©æå¤±å‡½æ•¸
loss_fn = nn.CrossEntropyLoss()

# ç¤ºä¾‹è¼¸å…¥ï¼šbatch_size=2, seq_len=4, num_classes=4 (B, I, E, S)
predictions = torch.randn(2, 4, 4)  # æ¨¡å‹è¼¸å‡ºçš„é æ¸¬å€¼
labels = trchTnsr([[0, 1, 2, 3], [1, 2, 0, 3]])  # å¯¦éš›æ¨™ç°½

# è¨ˆç®—æå¤±
loss = loss_fn(predictions.view(-1, 4), labels.view(-1))
2. CRFï¼ˆæ¢ä»¶éš¨æ©Ÿå ´ï¼‰ + CrossEntropyLoss
é©ç”¨å ´æ™¯ï¼šä¸­æ–‡åˆ†è©ä»»å‹™ä¸­ï¼Œå­—ç¬¦ä¹‹é–“æœ‰è¼ƒå¼·çš„ä¸Šä¸‹æ–‡ä¾è³´é—œç³»æ™‚ï¼Œå¯ä»¥ä½¿ç”¨æ¢ä»¶éš¨æ©Ÿå ´ï¼ˆCRFï¼‰ä¾†å»ºæ¨¡æ¨™æ³¨çš„åºåˆ—ä¾è³´ã€‚
ç‰¹é»ï¼š
CRF å¯ä»¥é€šéå»ºæ¨¡æ¨™ç°½ä¹‹é–“çš„è½‰ç§»æ¦‚ç‡ï¼Œæ•æ‰åºåˆ—æ¨™æ³¨ä»»å‹™ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¾è€Œå¾—åˆ°æ›´æº–ç¡®çš„åˆ†è©çµæœã€‚
é€šå¸¸åœ¨ LSTMã€BiLSTM æˆ– Transformer è¼¸å‡ºå±¤å¾ŒåŠ å…¥ CRF ä½œçˆ²æœ€å¾Œçš„è§£ç¢¼å±¤ï¼Œç„¶å¾Œä½¿ç”¨ CRF ç‰¹æœ‰çš„æå¤±å‡½æ•¸ã€‚
# å¦‚æœä½¿ç”¨ CRFï¼Œæå¤±å‡½æ•¸é€šå¸¸åœ¨ CRF æ¨¡å¡Šå…§å®šç¾©
# ä¾‹å¦‚ï¼štorchcrfåº«å¯ä»¥ä½¿ç”¨CRFå±¤ï¼Œå¹¶ä¸”éœ€è¦å°ˆé–€çš„æå¤±å‡½æ•¸
from torchcrf import CRF

crf = CRF(num_tags=4, batch_first=True)
logits = torch.randn(2, 4, 4)  # BiLSTMæˆ–Transformerçš„è¼¸å‡º
labels = trchTnsr([[0, 1, 2, 3], [1, 2, 0, 3]])  # å¯¦éš›æ¨™ç°½

# è¨ˆç®— CRF æå¤±
loss = -crf(logits, labels)
3. CTC Lossï¼ˆå¯é¸ï¼‰
é©ç”¨å ´æ™¯ï¼šå¦‚æœä½ çš„åˆ†è©ä»»å‹™æ¶‰åŠéå°é½Šçš„åºåˆ—ï¼Œä¾‹å¦‚è¼¸å…¥è¼¸å‡ºåºåˆ—é•·åº¦ä¸ä¸€è‡´ï¼ˆé¡ä¼¼èªéŸ³è­˜åˆ¥çš„å ´æ™¯ï¼‰ï¼Œå¯ä»¥è€ƒæ…® CTCLossã€‚
ç‰¹é»ï¼š
CTCLoss ä¸»è¦ç”¨äºè™•ç†è¼¸å…¥åºåˆ—å’Œè¼¸å‡ºæ¨™ç°½åºåˆ—ä¸å°é½Šçš„å ´æ™¯ï¼Œå°äºå¤§å¤šæ•¸æ¨™æº–çš„ä¸­æ–‡åˆ†è©å•é¡Œä¸å¤ªå¸¸ç”¨ï¼Œä½†å¦‚æœä½ çš„åˆ†è©ä»»å‹™æœ‰å°é½Šå•é¡Œï¼Œå®ƒæ˜¯ä¸€å€‹é¸æ“‡ã€‚
import torch.nn as nn

loss_fn = nn.CTCLoss()

# ç¤ºä¾‹è¼¸å…¥
input_lengths = trchTnsr([4, 4], dtype=torch.long)  # æ¯å€‹è¼¸å…¥åºåˆ—çš„é•·åº¦
target_lengths = trchTnsr([4, 3], dtype=torch.long)  # æ¯å€‹æ¨™ç°½åºåˆ—çš„é•·åº¦
predictions = torch.randn(4, 2, 5).log_softmax(2)  # æ¨¡å‹çš„é æ¸¬å€¼ (T, N, C)
labels = trchTnsr([[0, 1, 2, 3], [1, 2, 3, -1]])  # -1 è¡¨ç¤ºå¡«å……å€¼

# è¨ˆç®—æå¤±
loss = loss_fn(predictions, labels, input_lengths, target_lengths)
3. å°çµ CrossEntropyLoss æ˜¯ä¸­æ–‡åˆ†è©æœ€å¸¸ç”¨çš„æå¤±å‡½æ•¸ï¼Œå› çˆ²åˆ†è©ä»»å‹™æœ¬è³ªä¸Šæ˜¯åºåˆ—æ¨™æ³¨ä»»å‹™ï¼Œå±¬äºå­—ç¬¦ç´šåˆ¥çš„å¤šåˆ†é¡å•é¡Œã€‚
å¦‚æœå¸Œæœ›å¼•å…¥æ¨™ç°½ä¹‹é–“çš„ä¾è³´æ€§ï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨ CRF æ¨¡å‹é€²è¡Œè™•ç†ï¼Œå®ƒèƒ½æ•æ‰å­—ç¬¦é–“çš„é †åºé—œç³»ã€‚
å¦‚æœä»»å‹™éœ€è¦è™•ç†ä¸å°é½Šçš„åºåˆ—ï¼Œå‰‡å¯ä»¥é¸æ“‡ CTCLossï¼Œä½†é€™åœ¨ä¸­æ–‡åˆ†è©ä¸­çš„æ‡‰ç”¨è¼ƒå°‘ã€‚
ä½ å¯ä»¥æ ¹æ“šä»»å‹™çš„å…·é«”éœ€æ±‚ã€æ•¸æ“šé›†ä»¥åŠæ¨¡å‹çµæ§‹é¸æ“‡åˆé©çš„æå¤±å‡½æ•¸ã€‚é€šå¸¸æƒ…æ³ä¸‹ï¼Œç›´æ¥ä½¿ç”¨ CrossEntropyLoss å°±å¯ä»¥æ»¿è¶³ä¸­æ–‡åˆ†è©çš„éœ€æ±‚ã€‚å¦‚æœéœ€è¦æ›´è¤‡é›œçš„ä¸Šä¸‹æ–‡ä¾è³´ï¼Œå¯ä»¥åŠ å…¥ CRF å±¤ã€‚

== Pytorch bert åˆ¤æ–·å‰å¾Œå­— é€²è¡Œä¸­æ–‡æ–·è© ==
ä½¿ç”¨ BERT ä¾†é€²è¡Œä¸­æ–‡åˆ†è©æ˜¯ä¸€ç¨®ç¾ä»£æ–¹æ³•ï¼Œç‰¹åˆ¥æ˜¯åœ¨è¤‡é›œçš„èªè¨€æ¨¡å‹ä»»å‹™ä¸­ã€‚BERT å¯ä»¥é€šéå…¶å¼·å¤§çš„ä¸Šä¸‹æ–‡ç·¨ç¢¼èƒ½åŠ›ï¼Œåˆ¤æ–·è©èªçš„é‚Šç•Œã€‚BERT çš„æœ¬è³ªæ˜¯ä¸€å€‹åŸºäºè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆTransformerï¼‰çš„é è¨“ç·´èªè¨€æ¨¡å‹ï¼Œèƒ½å¤ æ•æ‰åˆ°æ–‡æœ¬ä¸­å‰å¾Œå­—ç¬¦ä¹‹é–“çš„é—œç³»ï¼Œå› æ­¤éå¸¸é©åˆç”¨äºä¸­æ–‡åˆ†è©ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ BERT æ¨¡å‹ä¾†å¯¦ç¾ä¸­æ–‡åˆ†è©çš„åŸºæœ¬æ­¥é©Ÿï¼š

1. æº–å‚™æ•¸æ“š ä½ éœ€è¦ä¸€å€‹å·²ç¶“æ¨™æ³¨å¥½çš„ä¸­æ–‡åˆ†è©æ•¸æ“šé›†ï¼ˆå¦‚ PKU æˆ– MSRA æ•¸æ“šé›†ï¼‰ã€‚é€šå¸¸ï¼Œåˆ†è©ä»»å‹™å¯ä»¥è¢«å®šç¾©çˆ²ä¸€å€‹åºåˆ—æ¨™æ³¨ä»»å‹™ï¼Œå°‡æ¯å€‹å­—ç¬¦æ‰“ä¸Šæ¨™ç°½ï¼Œå¸¸è¦‹çš„æ¨™ç°½é«”ç³»çˆ²ï¼š

B: è©çš„é–‹é ­ (Begin)
I: è©çš„ä¸­é–“ (Inside)
E: è©çš„çµå°¾ (End)
S: å–®å­—è© (Single)
å› æ­¤ï¼Œæ¯å€‹å­—ç¬¦æœƒæœ‰ä¸€å€‹å°æ‡‰çš„æ¨™ç°½ï¼Œé¡ä¼¼äºåºåˆ—æ¨™æ³¨å•é¡Œä¸­çš„ BIO æ¨™æ³¨ã€‚

2. å®‰è£æ‰€éœ€åº« ä½¿ç”¨ Huggingface çš„ transformers åº«ä¾†åŠ è¼‰é è¨“ç·´çš„ BERT æ¨¡å‹ï¼Œå¹¶çµåˆ PyTorch é€²è¡Œè¨“ç·´å’Œé æ¸¬ã€‚

pip install transformers
3. åŠ è¼‰é è¨“ç·´ BERT æ¨¡å‹
å¯ä»¥ä½¿ç”¨é è¨“ç·´çš„ä¸­æ–‡ BERT æ¨¡å‹ä¾†é€²è¡Œä¸­æ–‡åˆ†è©ä»»å‹™ã€‚Huggingface æä¾›äº†å¤šç¨®ä¸­æ–‡ BERT æ¨¡å‹ï¼Œä¾‹å¦‚ bert-base-chineseã€‚

from transformers import BertTokenizer, BertForTokenClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') # åŠ è¼‰é è¨“ç·´çš„BERTæ¨¡å‹å’Œåˆ†è©å™¨
model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=4)  # B, I, E, S å°æ‡‰çš„æ¨™ç°½
4. é è™•ç†æ•¸æ“š æˆ‘å€‘éœ€è¦å°‡è¼¸å…¥æ–‡æœ¬è½‰æ›çˆ² BERT æ¨¡å‹å¯ä»¥æ¥å—çš„æ ¼å¼ï¼Œå³å°‡æ¯å€‹å¥å­åˆ†è©çˆ²å­è©ï¼Œå¹¶æ˜ å°„åˆ°ç›¸æ‡‰çš„ IDï¼ŒåŒæ™‚å°æ¯å€‹å­—ç¬¦æ‰“ä¸Š B, I, E, S æ¨™ç°½ã€‚

sentence = "æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†" # ç¤ºä¾‹ä¸­æ–‡å¥å­

tokens = tokenizer.tokenize(sentence) # å°‡å¥å­åˆ†è©å¹¶æ˜ å°„åˆ°ID
input_ids = tokenizer.convert_tokens_to_ids(tokens)

# é€™è£å‡è¨­æ¯å€‹å­—ç¬¦éƒ½æœ‰ä¸€å€‹æ¨™ç°½ï¼Œæˆ‘å€‘æ‰‹å‹•å‰µå»ºæ¨™ç°½
# ä¾‹å¦‚ï¼Œå°äº "æˆ‘æ„›è‡ªç„¶èªè¨€è™•ç†"ï¼š
# "æˆ‘" -> B, "æ„›" -> S, "è‡ªç„¶" -> B E, "èªè¨€" -> B E, "è™•ç†" -> B E
labels = [0, 3, 0, 2, 0, 2, 0, 2]  # 0: B, 1: I, 2: E, 3: S

input_ids = trchTnsr([input_ids]) # è½‰æ›çˆ² PyTorch å¼µé‡
labels = trchTnsr([labels])

outputs = model(input_ids, labels=labels) # ç²å–æ¨¡å‹è¼¸å‡º 5. æ¨¡å‹å‰å‘å‚³æ’­ å°‡è¼¸å…¥åºåˆ—å‚³å…¥ BERT æ¨¡å‹ï¼Œè¨ˆç®—æå¤±å¹¶é€²è¡Œåå‘å‚³æ’­ã€‚
loss = outputs.loss
logits = outputs.logits
print(f"Loss: {loss.item()}")
6. è¨“ç·´æ¨¡å‹ å¯ä»¥ä½¿ç”¨æ¨™æº–çš„ PyTorch è¨“ç·´å¾ªç’°ä¾†è¨“ç·´æ¨¡å‹ã€‚

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=5e-5) # å®šç¾©å„ªåŒ–å™¨
for epoch in range(3):  # è¨“ç·´3å€‹epoch è¨“ç·´å¾ªç’°
    model.train()
    outputs = model(input_ids, labels=labels) # å‰å‘å‚³æ’­
    loss = outputs.loss

    loss.backward() # åå‘å‚³æ’­

    optimizer.step() # æ›´æ–°åƒæ•¸
    optimizer.zero_grad()
    print(f"Loss: {loss.item()}")

model.eval() #7. é æ¸¬åˆ†è©çµæœ è¨“ç·´å®Œæˆå¾Œï¼Œå¯ä»¥é€²è¡Œæ¨ç†é æ¸¬ã€‚

with torch.no_grad():   # ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦
    outputs = model(input_ids)
    logits = outputs.logits

predicted_labels = torch.argmax(logits, dim=-1).squeeze().tolist() # ç²å–æ¯å€‹å­—ç¬¦çš„é æ¸¬æ¨™ç°½
print(f"Tokens: {tokens}")
print(f"Predicted labels: {predicted_labels}")
8. å¾Œè™•ç† å°‡æ¨¡å‹è¼¸å‡ºçš„æ¨™ç°½åºåˆ—è½‰åŒ–çˆ²å¯¦éš›çš„åˆ†è©çµæœã€‚

# æ ¹æ“šé æ¸¬çš„æ¨™ç°½å°‡ tokens æ‹¼æ¥æˆè©
word_pieces = []
current_word = []
for token, label in zip(tokens, predicted_labels):
    if label == 0:  # B
        if current_word:
            word_pieces.append("".join(current_word))
        current_word = [token]
    elif label == 1:  # I
        current_word.append(token)
    elif label == 2:  # E
        current_word.append(token)
        word_pieces.append("".join(current_word))
        current_word = []
    elif label == 3:  # S
        word_pieces.append(token)

if current_word: word_pieces.append("".join(current_word))

print(" ".join(word_pieces)) # è¼¸å‡ºæœ€çµ‚åˆ†è©çµæœ
æ¨¡å‹é¸æ“‡ï¼šæˆ‘å€‘ä½¿ç”¨äº† bert-base-chinese æ¨¡å‹ä¾†è™•ç†ä¸­æ–‡åˆ†è©ä»»å‹™ã€‚
æ•¸æ“šè™•ç†ï¼šä¸­æ–‡åˆ†è©è¢«è¦–çˆ²åºåˆ—æ¨™æ³¨å•é¡Œï¼Œæ¯å€‹å­—ç¬¦æœƒè¢«åˆ†é…ä¸€å€‹æ¨™ç°½ã€‚
è¨“ç·´éç¨‹ï¼šé€šé PyTorch è¨“ç·´ BERT æ¨¡å‹ï¼Œä½¿ç”¨ Huggingface çš„ BertForTokenClassification é¡ä¾†å®Œæˆåˆ†è©ä»»å‹™ã€‚
é æ¸¬å’Œå¾Œè™•ç†ï¼šæ¨¡å‹è¼¸å‡ºé æ¸¬æ¨™ç°½å¾Œï¼Œéœ€è¦å°‡æ¨™ç°½æ˜ å°„å›å¯¦éš›çš„åˆ†è©çµæœã€‚
é€šéé€™ç¨®æ–¹å¼ï¼ŒBERT æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶åŸ·è¡Œä¸­æ–‡åˆ†è©ä»»å‹™ã€‚å¦‚æœä½ æœ‰æ›´å¤šçš„è¨“ç·´æ•¸æ“šï¼Œä¹Ÿå¯ä»¥å¾®èª¿ BERT æ¨¡å‹ä»¥æé«˜å…¶åˆ†è©æ€§èƒ½
