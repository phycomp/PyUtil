torch 共軛梯度法 Conjugate Gradient
共軛梯度法 (Conjugate Gradient, CG) 是一種用於求解線性系統 Ax=b 的迭代算法，尤其適用於大型稀疏對稱正定矩陣。它能夠高效地求解高維問題，通常用於機器學習中的優化問題，例如最小化二次型目標函數。在 PyTorch 中，雖然沒有直接提供現成的共軛梯度法函數，但可以基於矩陣運算和梯度計算機制來實現共軛梯度法。共軛梯度法在優化問題中常用於替代傳統梯度下降法，尤其是在二次優化問題中，如線性回歸、機器學習的 L2 正則化問題等。
PyTorch 中實現共軛梯度法 以下是基於 PyTorch 實現共軛梯度法的代碼示例。假設我們要求解問題 Ax=b，其中 A 是對稱正定矩陣。
import torch

def conjugate_gradient(Ax, b, x=None, tol=1e-10, max_iter=1000):
    """
    用共軛梯度法求解 Ax = b，其中 A 是一個對稱正定矩陣。

    參數:
    Ax: callable，計算矩陣 A 與向量 x 的乘積函數
    b: PyTorch Tensor，目標向量
    x: 初始向量，如果不提供則默認為 0 向量
    tol: 誤差容忍度
    max_iter: 最大迭代次數

    返回:
    x: PyTorch Tensor，近似解
    """
    if x is None:
        x = torch.zeros_like(b)
    
    r = b - Ax(x)
    p = r.clone()
    rs_old = torch.dot(r, r)

    for i in range(max_iter):
        Ap = Ax(p)
        alpha = rs_old / torch.dot(p, Ap)
        x += alpha * p
        r -= alpha * Ap
        
        rs_new = torch.dot(r, r)
        if torch.sqrt(rs_new) < tol:
            print(f"Converged after {i+1} iterations.")
            break
        
        p = r + (rs_new / rs_old) * p
        rs_old = rs_new

    return x

# 示例使用
# 假設我們有對稱正定矩陣 A 和向量 b
n = 10
A = torch.randn(n, n)
A = A.t().mm(A)  # 搆造對稱正定矩陣
b = torch.randn(n)

# 定義矩陣乘法 Ax
Ax = lambda x: A.mm(x.unsqueeze(1)).squeeze(1)

# 使用共軛梯度法求解 Ax = b
x = conjugate_gradient(Ax, b)

# 檢查結果
print("Residual:", torch.norm(A.mm(x.unsqueeze(1)).squeeze(1) - b))
解釋：
    • Ax: 傳遞矩陣 A 和向量 x 的乘法，Ax(x) 返回 A⋅x。
    • b: 給定的向量，代表目標向量。
    • x: 初始解向量，如果沒有提供，則從 0 向量開始。
    • tol: 容差，用於判斷算法是否收斂。
    • max_iter: 最大迭代次數。
步驟：
    1. 初始化殘差 r=b−Ax，并將方向向量 p 初始化為 r。
    2. 計算 Ap 并根據公式更新解向量 x。
    3. 更新殘差 r，并計算新殘差的模長。
    4. 若新殘差小於設定容差，算法結束。
    5. 否則，更新搜索方向 p，并重復以上步驟直到達到最大迭代次數或收斂。
共軛梯度法的特點：
    • 適用場景：該方法非常適合處理大規模稀疏線性系統，尤其是對稱正定矩陣的情形。
    • 內存友好：不需要顯式存儲矩陣 A，只需要能夠計算 A⋅x，因此適合大型稀疏矩陣的應用。
    • 收斂速度：理論上，共軛梯度法會在 n 步之內收斂，其中 n 是矩陣的大小，但通常在更少的迭代中即可達到可接受的精度。
PyTorch 的梯度優化：
在 PyTorch 的優化中，常見的 torch.optim 模塊提供了很多優化器，如 SGD、Adam，但它們并沒有直接實現共軛梯度法。共軛梯度法通常在一些二次優化問題中表現良好，如線性模型、L2 正則化問題等。這段代碼展示了如何在 PyTorch 中實現和使用共軛梯度法來解決稀疏對稱正定線性系統。
**********************************************
from torch import randn as trchRandn, randint as trchRndnint
from torch.nn import CrossEntropyLoss, Module, Linear, BatchNorm1d, ReLU
from torch.optim import Adam

class SimpleNet(Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = Linear(784, 256)
        self.bn1 = BatchNorm1d(256)  # 批歸一化層
        self.fc2 = Linear(256, 10)
        self.relu = ReLU()

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)  # 批歸一化
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = SimpleNet() # 實例化模型、損失函數和優化器
criterion = CrossEntropyLoss()
優化器 = Adam(model.parameters(), lr=0.001)

input_data = trchRandn(64, 784)     #假設是批次大小為64的輸入
labels = trchRndnint(0, 10, (64,))  #隨機生成標簽

output = model(input_data) # 訓練步驟示例
loss = criterion(output, labels)

優化器.zero_grad() # 反向傳播和優化
loss.backward()
優化器.step()
