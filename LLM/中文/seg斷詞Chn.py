#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Chinese word segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ctn94PEX-WjhX8btMyh07oC7SV2_1HxE

# Chinese Word Segmentation

## Download the data and Prepare training/test data
"""

#import requests
import sys

#remote_url = "https://raw.githubusercontent.com/hhhuang/nlp2019fall/master/word_segmentation/"

#r = requests.get(remote_url + "data/as_training.utf8", allow_redirects=True)
#open('as_training.utf8', 'wb').write(r.content)

#r = requests.get(remote_url + "data/as_testing_gold.utf8", allow_redirects=True)
#open('as_testing_gold.utf8', 'wb').write(r.content)

raw_train, raw_test = [], []
with open("斷詞文本.txt", encoding="utf8") as fin:
    for line in fin:
        raw_train.append(line.strip().split("　"))   # It is a full white space

with open("測試文本.txt", encoding="utf8") as fin:
    for line in fin:
        raw_test.append(line.strip().split("　"))   # It is a full white space

#print("Number of sentences in the training data: %d" % len(raw_train))
#print("Number of sentences in the test data: %d" % len(raw_test))

"""## Use jieba"""

#!pip install jieba

import jieba

#print(list(jieba.cut("".join(raw_test[0]))))

"""## Perform jieba on Simplified Chinese and restore the segmetnation to the original input. """

#!pip install hanziconv

from hanziconv.hanziconv import HanziConv

#print(list(jieba.cut(HanziConv.toSimplified("".join(raw_test[0])))))

def restore(text, toks):
    results = []
    offset = 0
    for tok in toks:
        results.append(text[offset:offset + len(tok)])
        offset += len(tok)
    return results

text = "".join(raw_test[0])
print('restore',restore(text, list(jieba.cut(HanziConv.toSimplified(text)))))
"""## Build Your Own Chinese Word Segmenter

Prepare training instances for sequence labeling by converting a list of words to a sequence of tags
"""

def words_to_tags(words):
    tags = []
    for word in words:
        if len(word) == 1:
            tags.append('S')
        else:
            for i in range(len(word)):
                if i == 0:
                    tags.append('L')
                elif i == len(word) - 1:
                    tags.append('R')
                else:
                    tags.append('M')
    return tags
    
train_X = []
train_Y = []

test_X = []
test_Y = []

for sent in raw_train:
    train_X.append(list("".join(sent)))  # Make the unsegmented sentence as a sequence of characters
    train_Y.append(words_to_tags(sent))
    
for sent in raw_test:
    test_X.append(list("".join(sent)))  # Make the unsegmented sentence
    test_Y.append(words_to_tags(sent))
    
print('test_X',test_X[0])
print('test_Y',test_Y[0])

"""Create a CRF model for word segmentation"""

#!pip install sklearn-crfsuite

#import sklearn_crfsuite
from sklearn_crfsuite import scorers, metrics, CRF

def extract_sent_features(x):
    sent_features = []
    for i in range(len(x)):
        sent_features.append(extract_char_features(x, i))
    return sent_features
    
def extract_char_features(sent, position):
    char_features = {}
    for i in range(-3, 4):
        if len(sent) > position + i >= 0:
            char_features['char_at_%d' % i] = sent[position + i]
    return char_features

crf_tagger = CRF(algorithm='lbfgs', min_freq=20, max_iterations=300, verbose=True)

feature_X = []
for x in train_X:
    feature_X.append(extract_sent_features(x))
crf_tagger.fit(feature_X, train_Y)

def segment(sent):
    tags = crf_tagger.predict_single(extract_sent_features(list(sent)))
    tokens = []
    tok = ""
    for ch, tag in zip(list(sent), tags):
        if tag in ['S', 'L'] and tok != "":
            tokens.append(tok)
            tok = ""
        tok += ch
    if tok:
        tokens.append(tok)
    return tokens
            
print('segment',segment("法國總統馬克宏已到現場勘災，初步傳出火警可能與目前聖母院的維修工程有關。"))

"""## Evaluation
Scorer for CWS
"""

def compare(actual_toks, pred_toks):
    i = 0
    j = 0
    p = 0
    q = 0
    tp = 0
    fp = 0
    while i < len(actual_toks) and j < len(pred_toks):
        if p == q:
            if actual_toks[i] == pred_toks[j]:
                tp += 1
            else:
                fp += 1
            p += len(actual_toks[i])
            q += len(pred_toks[j])
            i += 1
            j += 1
        elif p < q:
            p += len(actual_toks[i])
            i += 1
        else:
            fp += 1
            q += len(pred_toks[j])
            j += 1
    return tp, fp, len(actual_toks)
    
def score(actual_sents, pred_sents):
    tp = 0
    fp = 0
    total = 0
    for actual_toks, pred_toks in zip(actual_sents, pred_sents):
        tp_, fp_, total_ = compare(actual_toks, pred_toks)
        tp += tp_
        fp += fp_
        total += total_
    recall = float(tp) / total
    precision = float(tp) / (tp + fp)
    f1 = 2. * recall * precision / (recall + precision)
    return recall, precision, f1

"""Testing"""

pred = []
actual = []
for sent in raw_test:
    pred.append(segment("".join(sent)))
    actual.append(sent)
print('actual', actual[0], 'pred',pred[0])

print('score',score(actual, pred))

pred = []
actual = []
fout = open("jieba.out", "w")
for sent in raw_test:
    text = "".join(sent)
    r = list(jieba.cut(HanziConv.toSimplified(text)))
    r = restore(text, r)
    fout.write(" ".join(r) + "\n")
    pred.append(r)
    actual.append(sent)
print('actual2nd', actual[0],'pred2nd', pred[0])
print('score2nd',score(actual, pred))
#sys.exit()
