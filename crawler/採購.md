以下是使用 Python 开发法拍屋與政府採購數據爬蟲 + Streamlit 視覺化的實作指南，分為技術步驟與程式碼範例：

---

### **步驟 1：爬蟲開發**
#### 法拍屋爬蟲 (需注意網站反爬機制)
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

def crawl_judicial_auctions():
    url = "https://aomp109.judicial.gov.tw/judbp/wkw/WHD1A02.htm"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 解析表格數據 (需根據實際 HTML 結構調整)
    table = soup.find('table', {'class': 'table-class'})  # 替換實際 class
    rows = table.find_all('tr')
    
    data = []
    for row in rows[1:]:  # 跳過表頭
        cols = row.find_all('td')
        item = {
            '地點': cols[0].text.strip(),
            '拍賣日期': cols[1].text.strip(),
            '底價': cols[2].text.strip(),
            '狀態': cols[3].text.strip()
        }
        data.append(item)
    
    return pd.DataFrame(data)
```

#### 政府採購爬蟲
```python
def crawl_government_procurements():
    url = "https://web.pcc.gov.tw/pis/"
    # 使用 Selenium 處理動態加載（範例結構）
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.headless = True
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    
    # 解析動態內容
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()
    
    # 解析數據（根據實際結構調整）
    procurements = []
    items = soup.select('.procurement-item')  # 替換實際選擇器
    for item in items:
        procurements.append({
            '標案名稱': item.find('h3').text,
            '機關': item.find('.agency').text,
            '預算': item.find('.budget').text
        })
    
    return pd.DataFrame(procurements)
```

---

### **步驟 2：數據存儲**
```python
# 保存到 CSV
df_auctions = crawl_judicial_auctions()
df_procurements = crawl_government_procurements()

df_auctions.to_csv('auctions.csv', index=False)
df_procurements.to_csv('procurements.csv', index=False)
```

---

### **步驟 3：Streamlit 視覺化**
```python
import streamlit as st
import pandas as pd
import plotly.express as px

# 頁面設置
st.set_page_config(page_title="法拍屋與標案視覺化", layout="wide")

# 標題
st.title("🏠 法拍屋與政府標案數據儀表板")

# 數據加載
@st.cache_data  # 快取數據
def load_data():
    auctions = pd.read_csv('auctions.csv')
    procurements = pd.read_csv('procurements.csv')
    return auctions, procurements

auctions, procurements = load_data()

# 側邊欄過濾器
st.sidebar.header("篩選條件")
selected_city = st.sidebar.selectbox("選擇縣市", options=auctions['地點'].unique())

# 法拍屋地圖視覺化
st.header("法拍屋分佈地圖")
# 需將地址轉換為經緯度（此處為示意，實際需調用地理編碼 API）
auctions['lat'] = 25.0  # 假數據
auctions['lon'] = 121.5 # 假數據
st.map(auctions[['lat', 'lon']])

# 政府標案長條圖
st.header("政府標案預算分析")
fig = px.bar(procurements, x='機關', y='預算', title='各機關預算分布')
st.plotly_chart(fig)

# 數據表格展示
st.subheader("詳細數據")
st.dataframe(auctions)
```

---

### **步驟 4：進階功能擴展**
1. **反爬蟲處理**：
   - 使用 `time.sleep()` 控制請求頻率
   - 輪換 User-Agent
   - 使用代理 IP

2. **地理編碼**：
   ```python
   from geopy.geocoders import Nominatim
   geolocator = Nominatim(user_agent="my_app")
   
   def get_lat_lon(address):
       location = geolocator.geocode(address)
       return (location.latitude, location.longitude) if location else (None, None)
   ```

3. **自動更新**：
   ```python
   import schedule
   import time

   def auto_update():
       # 重新爬取並保存數據
       schedule.every(6).hours.do(crawl_and_save)
       while True:
           schedule.run_pending()
           time.sleep(1)
   ```

---

### **執行方式**
```bash
pip install streamlit requests beautifulsoup4 pandas plotly selenium
streamlit run app.py
```

---

### **注意事項**
1. 法拍屋網站結構可能隨時變更，需定期維護爬蟲
2. 政府採購網需檢查 robots.txt 合法性
3. 地圖功能需申請 Google Maps API 或改用其他地理編碼服務
4. 部署時建議使用 Docker 容器化

完整程式碼需根據實際網站結構調整解析邏輯，建議先手動檢查目標網頁 HTML 後再編寫爬蟲。