ä»¥ä¸‹æ˜¯ä½¿ç”¨ Python å¼€å‘æ³•æ‹å±‹èˆ‡æ”¿åºœæ¡è³¼æ•¸æ“šçˆ¬èŸ² + Streamlit è¦–è¦ºåŒ–çš„å¯¦ä½œæŒ‡å—ï¼Œåˆ†ç‚ºæŠ€è¡“æ­¥é©Ÿèˆ‡ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š

---

### **æ­¥é©Ÿ 1ï¼šçˆ¬èŸ²é–‹ç™¼**
#### æ³•æ‹å±‹çˆ¬èŸ² (éœ€æ³¨æ„ç¶²ç«™åçˆ¬æ©Ÿåˆ¶)
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

def crawl_judicial_auctions():
    url = "https://aomp109.judicial.gov.tw/judbp/wkw/WHD1A02.htm"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # è§£æè¡¨æ ¼æ•¸æ“š (éœ€æ ¹æ“šå¯¦éš› HTML çµæ§‹èª¿æ•´)
    table = soup.find('table', {'class': 'table-class'})  # æ›¿æ›å¯¦éš› class
    rows = table.find_all('tr')
    
    data = []
    for row in rows[1:]:  # è·³éè¡¨é ­
        cols = row.find_all('td')
        item = {
            'åœ°é»': cols[0].text.strip(),
            'æ‹è³£æ—¥æœŸ': cols[1].text.strip(),
            'åº•åƒ¹': cols[2].text.strip(),
            'ç‹€æ…‹': cols[3].text.strip()
        }
        data.append(item)
    
    return pd.DataFrame(data)
```

#### æ”¿åºœæ¡è³¼çˆ¬èŸ²
```python
def crawl_government_procurements():
    url = "https://web.pcc.gov.tw/pis/"
    # ä½¿ç”¨ Selenium è™•ç†å‹•æ…‹åŠ è¼‰ï¼ˆç¯„ä¾‹çµæ§‹ï¼‰
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    
    options = Options()
    options.headless = True
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    
    # è§£æå‹•æ…‹å…§å®¹
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()
    
    # è§£ææ•¸æ“šï¼ˆæ ¹æ“šå¯¦éš›çµæ§‹èª¿æ•´ï¼‰
    procurements = []
    items = soup.select('.procurement-item')  # æ›¿æ›å¯¦éš›é¸æ“‡å™¨
    for item in items:
        procurements.append({
            'æ¨™æ¡ˆåç¨±': item.find('h3').text,
            'æ©Ÿé—œ': item.find('.agency').text,
            'é ç®—': item.find('.budget').text
        })
    
    return pd.DataFrame(procurements)
```

---

### **æ­¥é©Ÿ 2ï¼šæ•¸æ“šå­˜å„²**
```python
# ä¿å­˜åˆ° CSV
df_auctions = crawl_judicial_auctions()
df_procurements = crawl_government_procurements()

df_auctions.to_csv('auctions.csv', index=False)
df_procurements.to_csv('procurements.csv', index=False)
```

---

### **æ­¥é©Ÿ 3ï¼šStreamlit è¦–è¦ºåŒ–**
```python
import streamlit as st
import pandas as pd
import plotly.express as px

# é é¢è¨­ç½®
st.set_page_config(page_title="æ³•æ‹å±‹èˆ‡æ¨™æ¡ˆè¦–è¦ºåŒ–", layout="wide")

# æ¨™é¡Œ
st.title("ğŸ  æ³•æ‹å±‹èˆ‡æ”¿åºœæ¨™æ¡ˆæ•¸æ“šå„€è¡¨æ¿")

# æ•¸æ“šåŠ è¼‰
@st.cache_data  # å¿«å–æ•¸æ“š
def load_data():
    auctions = pd.read_csv('auctions.csv')
    procurements = pd.read_csv('procurements.csv')
    return auctions, procurements

auctions, procurements = load_data()

# å´é‚Šæ¬„éæ¿¾å™¨
st.sidebar.header("ç¯©é¸æ¢ä»¶")
selected_city = st.sidebar.selectbox("é¸æ“‡ç¸£å¸‚", options=auctions['åœ°é»'].unique())

# æ³•æ‹å±‹åœ°åœ–è¦–è¦ºåŒ–
st.header("æ³•æ‹å±‹åˆ†ä½ˆåœ°åœ–")
# éœ€å°‡åœ°å€è½‰æ›ç‚ºç¶“ç·¯åº¦ï¼ˆæ­¤è™•ç‚ºç¤ºæ„ï¼Œå¯¦éš›éœ€èª¿ç”¨åœ°ç†ç·¨ç¢¼ APIï¼‰
auctions['lat'] = 25.0  # å‡æ•¸æ“š
auctions['lon'] = 121.5 # å‡æ•¸æ“š
st.map(auctions[['lat', 'lon']])

# æ”¿åºœæ¨™æ¡ˆé•·æ¢åœ–
st.header("æ”¿åºœæ¨™æ¡ˆé ç®—åˆ†æ")
fig = px.bar(procurements, x='æ©Ÿé—œ', y='é ç®—', title='å„æ©Ÿé—œé ç®—åˆ†å¸ƒ')
st.plotly_chart(fig)

# æ•¸æ“šè¡¨æ ¼å±•ç¤º
st.subheader("è©³ç´°æ•¸æ“š")
st.dataframe(auctions)
```

---

### **æ­¥é©Ÿ 4ï¼šé€²éšåŠŸèƒ½æ“´å±•**
1. **åçˆ¬èŸ²è™•ç†**ï¼š
   - ä½¿ç”¨ `time.sleep()` æ§åˆ¶è«‹æ±‚é »ç‡
   - è¼ªæ› User-Agent
   - ä½¿ç”¨ä»£ç† IP

2. **åœ°ç†ç·¨ç¢¼**ï¼š
   ```python
   from geopy.geocoders import Nominatim
   geolocator = Nominatim(user_agent="my_app")
   
   def get_lat_lon(address):
       location = geolocator.geocode(address)
       return (location.latitude, location.longitude) if location else (None, None)
   ```

3. **è‡ªå‹•æ›´æ–°**ï¼š
   ```python
   import schedule
   import time

   def auto_update():
       # é‡æ–°çˆ¬å–ä¸¦ä¿å­˜æ•¸æ“š
       schedule.every(6).hours.do(crawl_and_save)
       while True:
           schedule.run_pending()
           time.sleep(1)
   ```

---

### **åŸ·è¡Œæ–¹å¼**
```bash
pip install streamlit requests beautifulsoup4 pandas plotly selenium
streamlit run app.py
```

---

### **æ³¨æ„äº‹é …**
1. æ³•æ‹å±‹ç¶²ç«™çµæ§‹å¯èƒ½éš¨æ™‚è®Šæ›´ï¼Œéœ€å®šæœŸç¶­è­·çˆ¬èŸ²
2. æ”¿åºœæ¡è³¼ç¶²éœ€æª¢æŸ¥ robots.txt åˆæ³•æ€§
3. åœ°åœ–åŠŸèƒ½éœ€ç”³è«‹ Google Maps API æˆ–æ”¹ç”¨å…¶ä»–åœ°ç†ç·¨ç¢¼æœå‹™
4. éƒ¨ç½²æ™‚å»ºè­°ä½¿ç”¨ Docker å®¹å™¨åŒ–

å®Œæ•´ç¨‹å¼ç¢¼éœ€æ ¹æ“šå¯¦éš›ç¶²ç«™çµæ§‹èª¿æ•´è§£æé‚è¼¯ï¼Œå»ºè­°å…ˆæ‰‹å‹•æª¢æŸ¥ç›®æ¨™ç¶²é  HTML å¾Œå†ç·¨å¯«çˆ¬èŸ²ã€‚